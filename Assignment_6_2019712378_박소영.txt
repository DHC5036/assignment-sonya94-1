1. We learned optimizer such as SGD, Adagrad, RMSProp, Adadelta, and Adam. Research the other two state-of-the-art optimizers and explain their feature.
-> 



-------- Note -----------
- AdaBound 
- AdaMax
- AMSGrad
- CLR(Cycle Learning Rate)
- Nadam (Nesterov Adaptive Momentum)
- NAG (Nesterov accelerated gradient)
